{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUNoqkRVpV14"
   },
   "source": [
    "# Introduction to Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNQcidbEpV16"
   },
   "source": [
    "## What's the difference between Artificial Intelligence, Machine Learning and Deep Learning?\n",
    "\n",
    "**Artificial Intelligence** (AI) is a very old field of research. It started in the 1950s; at that time, the pioneers in AI were convinced that they could make machines think. The initial approach was attempting to model intellectual tasks, and program those tasks in computers. That is, in the beginning, AI was about people *hard-coding* problems that the machine tried to solve. A good example of an early AI program is computer chess. Early AI computer chess programs had **lots of rules pre-conceived by the program designers**. So although the term refers to \"intelligence\", these programs were not \"intelligent\", but just a sophisticated set of rules.\n",
    "\n",
    "* The term *neural network* was also born at that time. The initial idea was trying to mimic how a *neuron* works (or in fact, how it was thought a neuron worked when the term was coined) in the brain. As in the case of *Artificial Intelligence*, please do not be fooled by the terms. A *neural network* is just a way to compute approximation to functions. A powerful and useful way, true. But it is not any kind of \"artificial brain\", or anything like that.\n",
    "\n",
    "This approach is now obsolete. We now know that some tasks are so complex, that we will never be able to pre-design a system to have a good performance on those tasks. Take the example of the game of Go. Any program designed by humans to play Go was dull and could not compete against professional players. The game is so hard that is just impossible for us as humans to conceived a good Go computer player.\n",
    "* https://en.wikipedia.org/wiki/Go_(game)\n",
    "\n",
    "The modern approach to solve this kind of problems involves the use of data. This is what the *learning* part of *Deep Learning* (and *Machine Learning*) refers to. **Learning from data**. So, with this brief introduction, we can say that *Artificial Intelligence* is a broad concept that includes *Machine Learning*, that includes *Deep Learning*. *Artificial Intelligence* does not necessarily means learning from data, but Machine Learning does. And *Deep Learning* is just doing *Machine Learning* with neural networks.\n",
    "\n",
    "![](https://github.com/FasterUpperBeyond/deep-learning-images/blob/master/imgs/01_ai_ml_dl.png?raw=1)\n",
    "\n",
    "We will see during this course what *learning* means, and what is a *neural network*. \n",
    "\n",
    "In the case of the game of Go, all attempts to design a good Go computer player failed, until Google DeepMind created AlphaGo, a system that *learned* to play Go based on data from previous games. AlphaGo is an example that learning from data can provide computer systems that are superior to any system pre-conceived by humans.\n",
    "* Find out more about AlphaGo by watching this utterly interesting movie: https://www.alphagomovie.com/\n",
    "\n",
    "In this course, we will give our first steps on *Deep Learning* and *Neural Networks*, to start creating systems that perform complex tasks by learning from data. Instead of attempting to derive the rules by ourselves, those rules will be inferred by a neural network based on the data we provide:\n",
    "\n",
    "![](https://github.com/FasterUpperBeyond/deep-learning-images/blob/master/imgs/02_rules_data.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KF88KADpV19"
   },
   "source": [
    "## Neural Networks\n",
    "\n",
    "A neural network is just a stacked set of layers. Each layer is represented by a tensor, and it performs an affine transformation on each one of the layers.\n",
    "* A matrix is a 2D tensor\n",
    "* Affine transformation $\\vec{y} = W\\cdot\\vec{x} + \\vec{b}$, where $W$ is the weights tensor, $\\vec{x}$ is the input data, $\\vec{y}$ is the output data and $\\vec{b}$ is called the bias\n",
    "* $W$ and $\\vec{b}$ are model parameters that need to be found by training\n",
    "\n",
    "![](https://github.com/FasterUpperBeyond/deep-learning-images/blob/master/imgs/05_nn.png?raw=1)\n",
    "\n",
    "Note that this is a **linear operation**, therefore the result will be linear too. But the functions we want to learn are normally non-linear. How do we overcome this limiation? By using an **activation function**, that transforms the result from the linear operation. So, we actually do the following operation:\n",
    "\n",
    "$\\vec{y} = f\\left(W\\cdot\\vec{x} + \\vec{b}\\right)$\n",
    "\n",
    "There are several common activation functions, depending on the characteristics of the data and the kind of problem we want to solve. Here we will use the probably most often used activation function: **rectified linear unit**, or **relu** for friends:\n",
    "\n",
    "![Relu plot](https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg)\n",
    "\n",
    "(image extracted from https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "\n",
    "Relu will add the non-linearity that we need to learn arbitrary functions.\n",
    "\n",
    "We can achieve the same with many other activation functions. See the documentation of Keras for details on the available activation functions:\n",
    " - https://keras.io/activations/\n",
    "\n",
    "\n",
    "Each layer learns a new representation of the input data, and after several layers, the representation will make it straightforward to solve the task (e.g. assiging a digit to an image of a digit).\n",
    "\n",
    "Until very recently, any attempt to train a deep neural network would not provide good results. Recent progresses (e.g. advances in hardware, new activation functions) make it possible to train very deep network, and therefore, solve problems that were unfeasible not long ago.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPrPq5ibRZpR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adobCSwwRXce"
   },
   "source": [
    "\n",
    "### Training process\n",
    "\n",
    "The parameters of the network are initially randomly assigned. Then after each pass of the network, that is, after each *epoch*, the predictions of the network is compared against the true values, applying a *loss function*. This score is used to update the parameters of the network, with a process called **backpropagation**.\n",
    "* Backpropagation is a complex process that involves calculating the gradient of the parameters of the network. For more details see: https://en.wikipedia.org/wiki/Backpropagation\n",
    "* See also this excellent and visual explanation of how backpropagation works: https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/\n",
    "\n",
    "The training process is just an optimization process, that is normally done using a gradient descent-based algorithm. The values of gradients are computed using the backpropagation algorithm. Each epoch will make the loss score to be lower, and the predictions to be better.\n",
    "\n",
    "![](https://github.com/FasterUpperBeyond/deep-learning-images/blob/master/imgs/06_optim.png?raw=1)\n",
    "\n",
    "In practice, we have to fine tune the number of epochs (and other parameters), until we obtain a result that is good enough for our purpose. In fact, if the number of epochs is too high, our model may **overfit** the training data. In that case, it will perform worsely when the model is exposed to data that it  has not seen previously.\n",
    "* Fine tuning the number of epochs, and other parameters (e.g. the number and shape of the layers) is known as **hyperparameters optimization**, or **hyperparameters tuning**\n",
    "* The parameters of the model are the $W$ and $b$ values of each layer, and those are determined by training.\n",
    "\n",
    "## Hyperparameters tuning\n",
    "\n",
    "In summary, when we are trying to obtain a deep learning model, the process that we will follow is:\n",
    "* Decide on the architecture of the network (number of layers, type of each layer, size of each layer)\n",
    "* Decide on the activation functions on each layer\n",
    "* Decide on the parameters related to the optimization process (e.g. learning rate)\n",
    "* Decide on the loss function that we will use\n",
    "* Decide on the metrics that we will use to evaluate the performance of the model\n",
    "* Decide on the training parameters (number of epochs, batch size)\n",
    "* Decide on how to split the data between training, validation and test sets.\n",
    "\n",
    "We obtain feedback for those decisions with the results of the training applied to the validation set. We train once, change some of the hyperparameters, train again, decide if that's better, and we keep going until we have found a model that is satisfactory enough for our purposes.\n",
    "\n",
    "After that, we finally evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cGxkboKpV1_"
   },
   "source": [
    "## Our first neural network\n",
    "\n",
    "Let's start with an easy example. We are going to produce some synthetic data, and will try to predict the function that generates the data. We will add some noise, to make the problem more difficult.\n",
    "\n",
    "The function we are going to try to predict is\n",
    "\n",
    "$\\displaystyle y = 3*x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "91OPICEQpV2A",
    "outputId": "322ccc56-ecd9-41ac-9d9b-a45c9c5be514"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkeKnZRIqEvX"
   },
   "source": [
    "## Calling Tensorboard\n",
    "\n",
    "Tensorboard is a widely used extension that allows one to see the evolution of the cost funtion, metrics, weights and so on.\n",
    "\n",
    "We follow this notebook as reference:\n",
    "\n",
    "https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb\n",
    "\n",
    "Note that we have to restart keras internal graph (were models, layers, etc. are stored) when using this callback. For this, just add: `keras.backend.clear_session()`, before creating your model. \n",
    "\n",
    "In order to start a Tensorboard server, run: `%tensorboard --logdir logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvUPVjPIrRKP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9ejrBj0pV3Y"
   },
   "source": [
    "### Now another more complex function\n",
    "\n",
    "Let's try to fit a parabollic function\n",
    "\n",
    "$\\displaystyle y = -2x^2 + x + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_w86sgipV3a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLWJRAwCpV4U"
   },
   "source": [
    "### And now even a more complex one\n",
    "\n",
    "Let's try to predict now this function:\n",
    "\n",
    "$\\displaystyle y = \\cos(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4VyJBjFpV4V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01_Intro_Deep_Learning_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
