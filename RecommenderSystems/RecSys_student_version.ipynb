{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems\n",
    "\n",
    "## One of the most common uses of big data is to predict and suggest what users may want.  This allows Google to show you relevant ads or to suggest news in Google Now; Amazon to recommend relevant products; Netflix to recommend movies that you might like; or most recently, the famous **Weekly Dicovery** of Spotify.\n",
    "\n",
    "## All these products are based on systems of recommendation: a information retrieval method to provide users with relevant, yet novel and diverse, information. \n",
    "\n",
    "## In this class we will use a pretty famous dataset based on movies ratings, 'MovieLens', to learn the basics of recommender systems. \n",
    "\n",
    "## Table of Contents (times are approximated)\n",
    "\n",
    "1. [Getting and analysing some data (~1:30 h)](#data)\n",
    "2. [Most popular movies (~30 min)](#popular)\n",
    "3. [Metrics for recommender systems (~1.30h)](#metrics)\n",
    "4. [Collaborative Filtering (~15 min)](#cf)  \n",
    "   4.1 [Co-occurrence Matrix (~1.30h)](#copurchase)\n",
    "   <br></br>\n",
    "   4.2 [Memory-based CF (~1 h)](#memory-base)\n",
    "   <br></br>\n",
    "   4.3 [Model-based CF (~2 h)](#model-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 1.1 Load data\n",
    "\n",
    "We will use MovieLens dataset, which is one of the most common datasets used when implementing and testing recommender engines. This data set consists of:\n",
    "* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "* Each user has rated at least 20 movies. \n",
    "* Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n",
    "The data was collected through the MovieLens [website](https://movielens.org) during the seven-month period from September 19th, \n",
    "1997 through April 22nd, 1998. This data has been cleaned up - users\n",
    "who had less than 20 ratings or did not have complete demographic\n",
    "information were removed from this data set.\n",
    "\n",
    "You can download the dataset [here](http://files.grouplens.org/datasets/movielens/ml-100k.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the readme file!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pandas library is nothing alse than numpy under the hood (numpy with steroids, if you like). You can access the data (in matrix from) with he \"values\" attribute, e.g. data.values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access all rows, and first 3 columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access all collumns, and first 3 rows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access first 10 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access first column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute shape provides the shape of the matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that if we return the first column, we get a vector (of 100000 components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same with the first row (this time, we get a vector of 4 components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users and items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A dictionary for movies and a search tool\n",
    "\n",
    "In order to analyze the predicted recommendations, let's create a python dictonary that will allow us to translate any item id to the corresponding movie title. Also, let's write a small function that returns the ids of the movies containing some text.\n",
    "\n",
    "The correspondance between titles and ids is stored in the u.item file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Simple reminder of dictionaries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access value of key='hola'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update value of existing key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for movie titles and ids\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this dict to see the films a user has seen, for instance. \n",
    "for record in data.values[:20]:\n",
    "    print(\"User {u} viewed '{m}' and gave a {r} rating\".format(\n",
    "        u=record[0], m=item_dict[record[1]], r=record[2]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that retrieves all the ids and titles for movies containing 'text' in its title\n",
    "def returnItemId(text, ids):\n",
    "    \"\"\"\n",
    "    :param text: string to be looked for in movies titles\n",
    "    :param ids: dicttionary of {id:title}\n",
    "    \n",
    "    :return: a list of (id,title) if text found in titles, and an empty list otherwise.\n",
    "    \"\"\"\n",
    "    # convert input text to lowercase\n",
    "\n",
    "    # find occurances\n",
    "    \n",
    "    # Get the IDs corresponding to the given text\n",
    "    \n",
    "    # Return a list with the id and the name\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data consistency (always double check everything!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether titles are unique or not. They are not!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One work around: create another dict that consolidates ids with the same movie title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las claves en \"duplicates_item_dict\" son los nombres de las películas\n",
    "# Los valores son una lista de los ids (que pueden ser uno solo, o varios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dict where the key are the original ids, and the values are the unique one. \n",
    "We will use this dictionary to remove duplicates in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another dict mapping moving titles to this new unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our `returnItemId()` mehtod safely =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Train and test sets\n",
    "\n",
    "GroupLens provides several splits of the dataset, so that we can check the goodness of our algorithms. See the README file for more  details. Here we will use one of such splits.\n",
    "\n",
    "Please notice that we have to correct for the non-unique movie's id issue!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting for non-unique movies id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder of lambda functions in Python: is a way of calling short functions (1 line of code), without having to define the function in a separted cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These three  lines of code are equivalent!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='popular'></a>\n",
    "## 2. Most popular movies\n",
    "\n",
    "Recommending popular items is a simple, yet quite effective baseline for recommendation. Indeed, most RS suffer from a strong *popularity bias*, i.e. they tend to recommend popular items more frequently than they should -just because suggesting what is popular is effective!-. There is a lot of research  devote to understand this behaviour and to develop recipies to avoid it. \n",
    "\n",
    "Movies can be ranked according to different popularity metrics:\n",
    "* Most rated movie (it is assumed that this is the most watched movie)\n",
    "* Most positively rated movie (rating > 4.0)\n",
    "* Highest rated movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Most rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the train dataset by item and count the number of users using Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a numpy array of [id, title, frequency]\n",
    "\n",
    "# numpy requires knowledge of the data types. Since we have ids (integers) and titles (strings), \n",
    "# we will use a \"parent\" data type, np.object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Most positively rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter movies rated with rating >=4.0. Then group by item, count the number of users and sort in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a numpy array of [id, title, frequency]\n",
    "\n",
    "# numpy requires knowledge of the data types. Since we have ids (integers) and titles (strings), \n",
    "# we will use a \"parent\" data type, np.object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Highest mean rating movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaine the highest rated movies, with a minium number of users/ratings.\n",
    "\n",
    "# group the ratings by item and stack them in a list\n",
    "\n",
    "# filter movies with a minimum number of ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the mean of the list of rating per movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a numpy array of [id, title, frequency]\n",
    "\n",
    "# numpy requires knowledge of the data types. Since we have ids (integers) and titles (strings), \n",
    "# we will use a \"parent\" data type, np.object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: set the value of *min_ratings* to 1, and re-run the cell. What happens now? Change this value\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: Which method is better?? How to measure a recommender system? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** IMPORTANT QUESTION **: When might be useful to recommend popular items?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='metrics'></a>\n",
    "## 3. Metrics for recommender systems\n",
    "\n",
    "As we have seen, even with the simplest solution --aka, recommending popular items-- is difficult to known which technique performs better. For this, there are a number of metrics that allow one to measure the goodness of a recommender system. \n",
    "\n",
    "Metrics can be design for measuring the relevance or accuracy of a recommendation, but they can be created for evaluating the novelty of a recommendation, or its diversity. \n",
    "\n",
    "For now, we will focus on relevance and accuracy. Several metrics exist:\n",
    "* Accuracy: rmse, mae.\n",
    "* Not ranked: Recall@k, Precision@k.\n",
    "* With rank disccount: map@k, ndcg@k.\n",
    "* With rank ordering: mean percentile rank.\n",
    "\n",
    "We will be definiing some of them whitin this class. For the moment, let's talk about precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Precision and recall\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precision and Recall in IR\" style=\"float: right; width: 300px\"/>\n",
    "\n",
    "The concept of precision and recall comes form the world of information retrieval, have a look at the wikipedia:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "From this entry:\n",
    "\n",
    " * \"**precision** (also called positive predictive value) is the fraction of retrieved instances that are relevant\".\n",
    " * \"**recall** (also known as sensitivity) is the fraction of relevant instances that are retrieved\".\n",
    "\n",
    "<br />\n",
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: how do we know if some movie, unknown to the user, is relevant?\n",
    "</div>\n",
    "\n",
    "In other words, we cannot measure a false positive --something recommended that was not relevant--. In this regard, only recall-oriented metrics have an actual meaning in RS. Nonetheless, its common practice to define both metrics in RS as follows:\n",
    " \n",
    "### $$\\mathrm{recall}@N = \\frac{\\sum_{k=1}^N rel(k)}{\\sum_{i\\in \\mathcal{I}_u} 1}$$\n",
    "### $$\\mathrm{precision}@N = \\frac{\\sum_{k=1}^N rel(k)}{N}$$\n",
    "\n",
    "Here, $\\mathcal{I}_u$ is the set of items adopted by user $u$, and $rel(k)$ is the relevance of a recommendation at position k in the list of recommendations. For ratings, the relevance could be defined as those movies rated above a certain threshold, e.g. $r_{ui}>4.0$. \n",
    "\n",
    "**Important to note: since precision is pretty much the same as recall in RS, metrcis usch as the *area under the ROC curve* doesn't have any meaning!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "As an example, consider a user that watched the following films:\n",
    "<br /><br />\n",
    "'Designated Mourner, The (1997)'\n",
    "<br />\n",
    "'Money Talks (1997)'\n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br />\n",
    "'Batman Forever (1995)'\n",
    "<br /><br />\n",
    "The recommended items were: \n",
    "<br /><br />\n",
    "'Batman (1989)' \n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br /><br />\n",
    "**What would be the recall and precision @1? and @2?**\n",
    "<br />\n",
    "**What do you think of recommending Batman? Is a bad or a good recommendation?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that there isn't any actual difference between precision and recall in the context of RS: both measure the relevance of the recommendations, and tell nothing about items recommended that haven't been adopted by the user. Thus, it make sense to define a normalized recall as:\n",
    "\n",
    "### $$\\mathrm{recall}@N = \\frac{\\sum_{i=1}^N rel_i}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1})$$\n",
    "\n",
    "This way, results are normalized to 1 always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**Exercise** Implement the above definition of recall\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_n(N, test, recommended, train=None):\n",
    "    \"\"\"\n",
    "    :param N: number of recommendations\n",
    "    :param test: list of movies seen by user in test\n",
    "    :param train: list of movies seen by user in train. This has to be removed from the recommended list \n",
    "    :param recommended: list of movies recommended\n",
    "    \n",
    "    :return the recall\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = ['Designated Mourner, The (1997)', 'Money Talks (1997)', 'Madame Butterfly (1995)', 'Batman Forever (1995)']\n",
    "recommended = ['Batman (1989)', 'Madame Butterfly (1995)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_at_n(1, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_at_n(2, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it's well normalized\n",
    "print(recall_at_n(3, seen, recommended))\n",
    "print(recall_at_n(10, seen, recommended))\n",
    "print(recall_at_n(100, seen, recommended))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, use this implementation to measure the efficiency of the popularity baselines in the test set. Use the top-5 movies, for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since `recall_at_n` takes both train and test list per user, we need to create a dataset with the list of movies seen in train and test*\n",
    "\n",
    "Thus, get the list of movies per user in train and test, and join the two dataframes. For the join, use the pandas method `merge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movies in train per user. For this, group by user and get a list of item ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same with test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also access test values as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This second method is easier if we want to access several columns at once, and operate over them.\n",
    "# For instance, if we like to concatenate both train and test list, we will do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the recall of the mostRatedMovies recommendation, for each user:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As you can see, some users have a quite large recall (0.5), while for others is small (e.g, 0.14). Let's calculate the mean.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average recall across all users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Mean Averaged Precision (MAP) -- Advanced material\n",
    "\n",
    "Previous metrics did not account for the ranking of the recommendation, i.e. the relative position of a movie within the sorted list of recommendations. **But orders matters!** Metrics like MAP, MRR or NDCG try to tackle down this problem. \n",
    "\n",
    "From the blog *http://fastml.com/what-you-wanted-to-know-about-mean-average-precision/*:\n",
    "\n",
    "> Here’s another way to understand average precision. Wikipedia says AP is used to score document retrieval. You can think of it this way: you type something in Google and it shows you 10 results. It’s probably best if all of them were relevant. If only some are relevant, say five of them, then it’s much better if the relevant ones are shown first. It would be bad if first five were irrelevant and good ones only started from sixth, wouldn’t it? AP score reflects this.\n",
    "\n",
    "Implementation taken from:\n",
    "\n",
    "https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision \n",
    "\n",
    "The Average Precision is definied as:\n",
    "\n",
    "### $$\\mathrm{AP}@N = \\frac{\\sum_{k=1}^N P(k) \\times rel(k)}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1)}$$\n",
    "\n",
    "where $P(k)$ is the precision at cut-off in the item list, i.e. the ratio of the number of recommended items adopted, up to the position k, over the number k. Thus:\n",
    "\n",
    "### $$\\mathrm{AP}@N = \\frac{\\sum_{k=1}^N \\left(\\sum_{i=1}^k rel(i)\\right)/k \\times rel(k)}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Following the example above, consider a user that watched the following films:\n",
    "<br /><br />\n",
    "'Designated Mourner, The (1997)'\n",
    "<br />\n",
    "'Money Talks (1997)'\n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br />\n",
    "'Batman Forever (1995)'\n",
    "<br /><br />\n",
    "The recommended items were: \n",
    "<br /><br />\n",
    "'Batman (1989)' \n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**Calculate AP@1**\n",
    "<br /><br />\n",
    "First, *rel(1)=0*, because Batman was not viewed. Also, *P(1) = 0*. Thus, AP@1=0.\n",
    "<br />\n",
    "**Calculate AP@2**\n",
    "<br /><br />\n",
    "As before, *rel(1)=0*, so the first term does not contribute. For the second term, *rel(2)=1*, so that *P(2)=0.5*. The numerator is hence:\n",
    "<br /><br />\n",
    "$P(1)*rel(1)+P(2)*rel(2)=0*0+0.5*1$\n",
    "<br /><br />\n",
    "For the denominator, $N=2$ and $\\sum_{i\\in \\mathcal{I}_u} 1)=4$, thus:\n",
    "<br /><br />\n",
    "AP@2 = 0.5/2 = 0.25\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement it =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(N, test, recommended, train=None):\n",
    "    \"\"\"\n",
    "    Computes the average precision at N given recommendations.\n",
    "    \n",
    "    :param N: number of recommendations\n",
    "    :param test: list of movies seen by user in test\n",
    "    :param train: list of movies seen by user in train. This has to be removed from the recommended list \n",
    "    :param recommended: list of movies recommended\n",
    "    \n",
    "    :return The average precision at N over the test set\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = ['Designated Mourner, The (1997)', 'Money Talks (1997)', 'Madame Butterfly (1995)', 'Batman Forever (1995)']\n",
    "recommended = ['Madame Butterfly (1995)', 'Batman (1989)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apk(1, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apk(2, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apk(3, seen, recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP\n",
    "\n",
    "Mean avergae precision is nothing else than the AP averaged across users ;)\n",
    "\n",
    "Apply it to popularity baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif\" alt=\"collaborative filtering\" style=\"float: right; width: 300px\"/>\n",
    "\n",
    "## 4. Collaborative Filtering <a id='cf'></a>\n",
    "\n",
    "Perhaps, one of the most succesful techniques for making personalized recommendations are the so called *collaborative filtering* (CF) algorithms. CF is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue X than to have the opinion on X of a person chosen randomly. \n",
    "\n",
    "The image at the right (from Wikipedia) shows an example of user's preference prediction using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in the image at the right the system has made a prediction, that the active user will not like the video.\n",
    "\n",
    "In this part we will see three kinds of CF, of increasing complexity:\n",
    "\n",
    "4.1 [CF with co-occurrence](#copurchase)\n",
    "\n",
    "4.2 [Memory-based CF](#memory-base)\n",
    "\n",
    "4.3 [Model-based CF](#model-base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='copurchase'></a>\n",
    "## 4.1 Co-occurrence Matrix\n",
    "\n",
    "The idea is to recommend movies similar to the movies already seen by a user. A measurement of similarity among items is obtained from the co-occurrence matrix. This is nothing else than the adjacency matrix of the graph of items created by users!!!\n",
    "\n",
    "<table border=\"0\" style=\"width:825px;border:0px;\">\n",
    "<tr>\n",
    "    <td> \n",
    "        <img src=\"https://lucidworks.com/wp-content/uploads/2015/08/Les-Miserables-Co-Occurrence.png\" style=\"width: 500px\"/>\n",
    "    </td>\n",
    "    <td> \n",
    "        <img src=\"https://lucidworks.com/wp-content/uploads/2015/08/midnight-club-graph.png\" style=\"width: 400px\"/>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of movies per user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of items in train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co-ocurrance matrix will have shape=[n_items,n_items]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**QUESTION:** Can you think of a better way of visualizaing this matrix? Try to rescale it, or to rearrenge it follwoing some criteria (for instance, popularity!).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Making predictions using the co-occurrence matrix\n",
    "\n",
    "This kind of recommendations, based on item similarity, provide a measure of the closeness of one item to another. In order to make a recommendation for a user, we have to proceed as follows:\n",
    "\n",
    "* First, define a function that returns the top-N closest items to a given one.\n",
    "* Then, for a list of items adopted by a specific user, select the top-N items from the lists of top-N closest items to each adopted item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrance_similarity(item_id, coocurrance, ntop=10):\n",
    "    \"\"\"\n",
    "    Returns the top-N most similar items to a given one, based on the coocurrance matrix\n",
    "    \n",
    "    :param item_id: id of input item\n",
    "    :param cooccurrance: 2-dim numpy array with the co-occurance matrix\n",
    "    :param ntop: number of items to be retrieved\n",
    "    \n",
    "    :return top-N most similar items to the given item_id\n",
    "    \"\"\"\n",
    "\n",
    "    # return indeces of most similar items in descendign order\n",
    "    # remove the first element, as it is the item itslef\n",
    "    \n",
    "    # return a numpy array with the index (first column) and the value (second column) of the most similar items\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column are indices, while second one is the frequency of co-ocurrance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with he movie ID!!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let use this function to make recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrance_recommendation(items_id, cooccurrance, ntop=10):\n",
    "    \"\"\"\n",
    "    Obtain the list of ntop recommendations based on a list of items (user history of views)\n",
    "    \n",
    "    :param items_id: list of items ids\n",
    "    :param coocurrence: co-ocurrence matrix (numpy 2-dim array)\n",
    "    :param ntop: top-K items to be retrieved\n",
    "    \n",
    "    :return list of ntop items recommended\n",
    "    \"\"\"\n",
    "    # put together all the similar items and its value. For this, use np.vstack, wich stacks one array after \n",
    "    # another (row wise)\n",
    "    # Group by id and take the maximum frquency to remove duplicates\n",
    "    \n",
    "    # sort by value in descending order\n",
    "    \n",
    "    # get the top N\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get users in train with their movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the recommendations for all users using the apply method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that, unlike previous popularity based models, the recommendation is now (slightly) different from one user to another**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (seen, recom) in zip(testUsersGrouped.values[:3, 1], predictions[:3]):\n",
    "    print(\"*\"*6)\n",
    "    print(\"Seen items: \")\n",
    "    print([unique_item_dict[i] for i in seen])\n",
    "    print(\"Recommended items: \")\n",
    "    print([unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute the recommendation\n",
    "\n",
    "For this, first add a column to `trainUserGrouped` with the predictions using the apply procedure above.\n",
    "Next, join this dataframe with the test dataframe, and get the `recall_at_n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a prediction column to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the df with train and predictions with the test df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to see the help of method recall_at_n\n",
    "\n",
    "# recall_at_n??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average recall across all users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average recall across all users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for different top k values. It might be convenient to define a function!\n",
    "def evaluate_recall(topN, trainGrouped, testGrouped, coMatrix, popularity_baseline):\n",
    "    # add a prediction column to train\n",
    "    \n",
    "    # join with test data\n",
    "\n",
    "    # calculate average recall\n",
    "    \n",
    "    \n",
    "    # calculate average recall for the baseline\n",
    "      \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3,10,30,50,100]:\n",
    "    evaluate_recall(k, trainUsersGrouped, testUsersGrouped, coMatrix, positiveRatedMovies[:, 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do the same analysis for the map metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to see the help of method apk\n",
    "\n",
    "# apk??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_map(topN, trainGrouped, testGrouped, coMatrix, popularity_baseline):\n",
    "    # add a prediction column to train\n",
    "    \n",
    "    # join with test data\n",
    "\n",
    "    # calculate average recall\n",
    "    \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3,10,30,50,100]:\n",
    "    evaluate_map(k, trainUsersGrouped, testUsersGrouped, coMatrix, positiveRatedMovies[:, 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "Compare this results to those obtained with the popularity model. Was it so bad?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments:\n",
    "* The dataset we are using here is very simple. Indeed, you can see that the popularity baseline achieves quite decent metric values. This won't happen in a real world dataset! The reason it happens here is because the dataset is quite small, and quite biased towards popular items. \n",
    "\n",
    "* Recall does not account for the order of the recommendation, while map does. This explains why the co-occurrance model performs better after the first 10 recommendations in terms of map (ordering), while recall values are always better for the popularity based model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Oher distances\n",
    "\n",
    "So far, we have defined the *closeness* of two items as the number of users shared. However, it would make make sense to define it relative the total number of users that have watch a movie. This can be done with the [Jaccard similarity index](https://en.wikipedia.org/wiki/Jaccard_index):\n",
    "\n",
    "$$J(i,j)=\\frac{|i\\cap j|}{|i|+|j|-|i\\cap j|}\\in [0,1]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Build the Jaccard similarity matrix from the co-occurrance matrix. Notice that $CoM(i,j) = |i\\cap j|$ and $CoM(k,k) = |k|$. In addition, if both $|i|=0$ and $|j|=0$, the similarity is defined as 1 (this is a convention).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the diagonal of CoMatrix provides the number of visualizations of each movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the matrix\n",
    "plt.matshow(jaccard, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "cax = plt.matshow(jaccard_sorted_total, fignum=1000, cmap=plt.cm.coolwarm)\n",
    "plt.gcf().colorbar(cax, ticks=[0, 0.1, 0.2, 0.25])\n",
    "plt.clim(0, 0.25)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print out the first Ntop recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predictoins with Jaccard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (seen, recom) in zip(testUsersGrouped.values[:3, 1], predictions[:3]):\n",
    "    print(\"*\"*6)\n",
    "    print(\"Seen items: \")\n",
    "    print([unique_item_dict[i] for i in seen])\n",
    "    print(\"Recommended items: \")\n",
    "    print([unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3,10,30,50,100]:\n",
    "    evaluate_recall(k, trainUsersGrouped, testUsersGrouped, jaccard, positiveRatedMovies[:, 0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3,10,30,50,100]:\n",
    "    evaluate_map(k, trainUsersGrouped, testUsersGrouped, jaccard, positiveRatedMovies[:, 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the co-occurance model with Jaccard distance works better than the one counting occurances. Indeed, it has higher recall values than the popularity model for large top-N**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='memory-base'></a>\n",
    "## 4.2. Memory-Based Collaborative Filtering (CF)\n",
    "\n",
    "Although the methods developed so far return a list of recommended items, they cannot be used to make an actual prediction regarding the rating. A quite different approach would be to calculate the unknown rating, $r_{ui}$, as the averaged of some other ratings, thta are somehow close to either the user or the item in question. \n",
    "\n",
    "Thus, one approach is to take\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K}\\sum_{j\\in\\mathcal{I}'} \\mathrm{sim}(i,j) r_{u,j},$$\n",
    "\n",
    "where items $j\\in\\mathcal{I}'$ are taken from the set of $K$ closest items to $i$, or from the whole dataset. This is known as **item-item collaborative filtering**, and can be interpreted as *“users who liked this movie also liked …”*. See Amazon famous patent: https://www.google.com/patents/US7113917. Basically, this technique will take an item, find users who liked that item, and find other items that those users or similar users also liked. \n",
    "\n",
    "Similarly, one can define a **user-user filtering** where predictions are made as\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K} \\sum_{v\\in\\mathcal{U}'} \\mathrm{sim}(u,v) r_{v,i}.$$\n",
    "\n",
    "<img src=\"https://soundsuggest.files.wordpress.com/2013/06/utility_matrix.png\" alt=\"utility matrix\" style=\"float: right; width: 400px\"/>\n",
    "\n",
    "In this case, the recommendation would be more like *“users who are similar to you also liked …”*. Both techniques are part of the broad familiy of **Memory-Based Collaborative Filtering** approaches, or neighborhood-based algorithms.\n",
    "\n",
    "The similarity among users or items can be calculated in a variety of forms: Pearson's correlation, cosine distance, etc. Here we will use the cosine distance. For this, we will first create the utility user-item matrix. \n",
    "\n",
    "The utility matrix is a dense representation of the user-item intearction. We have been using the *long* format, where missing entries are obviated; now, we will use the *wide* format, i.e. the matrix representation (see the figure on the right). \n",
    "\n",
    "<br></br>\n",
    "<div class = \"alert alert-info\">\n",
    "** NOTE **: Long and wide formats have its benefits and drawbacks. Can you think of some of them?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the train and test datasets in wide format (i.e., like a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility matrix training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility matrix testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a similarity measure: cosine similarity\n",
    "\n",
    "### $$\\mathrm{sim}({\\bf a},{\\bf b})=\\frac{{\\bf a}\\cdot{\\bf b}}{\\sqrt{{\\bf a}\\cdot{\\bf a}}\\sqrt{{\\bf  b}\\cdot{\\bf b}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(ratings, kind='user', epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Calculate the cosine distance along the row (columns) of a matrix for users (items)\n",
    "    \n",
    "    :param ratings: a n_user X n_items matrix\n",
    "    :param kind: string indicating whether we are in mode 'user' or 'item'\n",
    "    :param epsilon: a small value to avoid dividing by zero (optional, defaults to 1e-9)\n",
    "    \n",
    "    :return a square matrix with the similarities\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosineSimilarity(uMatrixTraining, 'item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. User-user CF\n",
    "\n",
    "*“Users who are similar to you also liked …”*\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K} \\sum_{v\\in\\mathcal{U}'} \\mathrm{sim}(u,v) r_{v,i}.$$\n",
    "Consider user $x$:\n",
    "\n",
    "1. Find other users whose ratings are “similar” to $x$’s ratings, i.e. calculate the similarity among users\n",
    "2. Estimate missing ratings based on ratings of similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cax = plt.matshow(userSimilarity, fignum=1000, cmap=plt.cm.coolwarm)\n",
    "plt.gcf().colorbar(cax, ticks=[0, 0.25, 0.5])\n",
    "plt.clim(0, 0.5)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that if we multiply `userSimilarity` by `uMatrixTraining` we get the ratings weigthed with user similar similarity. Then, we have to normalize by the average similarity for each user*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that some users might give generally lower ratings than others, so that we could have also corrected for this effect as follows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ = uMatrixTraining.sum(axis=1)\n",
    "len_ =np.count_nonzero(uMatrixTraining, axis=1)\n",
    "average_ratings = np.tile(sum_/ len_, n_items).reshape([n_items, n_users]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uMatrixTraining_shifted = uMatrixTraining - np.multiply(average_ratings, uMatrixTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userItemCFpredictions_corrected = average_ratings + userSimilarity.dot(uMatrixTraining_shifted) / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now rating values are more reasonable\n",
    "np.max(userItemCFpredictions_corrected), np.min(userItemCFpredictions_corrected), np.mean(userItemCFpredictions_corrected), np.std(userItemCFpredictions_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Item-Item CF\n",
    "\n",
    "*“Users who liked this movie also liked …”*\n",
    "\n",
    "Consider item $i$:\n",
    "\n",
    "1. For item $i$, find other similar items, i.e. calculate the similarity among items\n",
    "2. Estimate rating for item $i$ based on ratings for similar items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#itemItemCFpredictions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Show some recommendations\n",
    "\n",
    "In case of item-item CF, the recommendation is pretty much the same as with the co-occurence matrix. It's also quite simple to find similar items to a given one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Find movies similar to a given one using the item-item similarity matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Calculate the recommendations obtained with the item-item CF model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove relevant items seen in train from our prediction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Do the same with the user-user CF model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, the recommendations are pretty bad... Let's measure that**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Measure the recommendations\n",
    "\n",
    "Since we are predicting ratings, it might make sense to introduce a metric that accounts for this. In particular, the **Root Mean Square Error (RMSE)** is typically used for this purpose. \n",
    "\n",
    "### $$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n_{\\mathrm{users}}n_{\\mathrm{items}}}\\sum_{u,i}\\left(r_{u,i}-\\hat{r}_{u,i}\\right)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    \"\"\"\n",
    "    Return the Root Mean Squared Error of the prediction\n",
    "    \n",
    "    :param prediction: a 2-dim numpy array with the predictions\n",
    "    :param ground_truth: a 2-dim numpy array with the known ratings\n",
    "    \n",
    "    :return the RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User-based CF RMSE=%.3f' %rmse(userItemCFpredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User-based (shifted) CF RMSE=%.3f' %rmse(userItemCFpredictions_corrected, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Item-based CF RMSE=%.3f' %rmse(itemItemCFpredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\">\n",
    "**IMPORTANT TO NOTE**: RMSE was used in the RecSys community for many years to measure the accuracy \n",
    "of recommendations. However, it was demonstrated that high accuracy in predicting rating does not imply a good\n",
    "ranked list!!    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ranking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together the rows of the test, train and predicted item ids. This can be done easily with the zip command.\n",
    "# Be careful, though, because zip returns an iterable, so that it will dissapear once it's consumed. \n",
    "# To avoid this behavior, convert it to a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the zipped list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "As explained in class, the curse of dimensionality impedes us to obtain a good similarity measure among users or items (in this excercise, dimensions are ~1000; in a real Recommender Systems, they could be millions, billions or evene trillions!). This is the reason the above implementations do not work.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-base'></a>\n",
    "## 4.3. Model-based CF or Latent factor models\n",
    "There are several model-based CF: from matrix factorizations to bayesian models, neural netwroks, etc. In all of them, we try to extract latent factors (vectors) that model user and item interactions. In contrast to previous methods, our hypothesis here is that the dimension of the latent spaces is rather small (in the order of a hundred dimensions). Then, we use this latent features to make a prediction:\n",
    "\n",
    "## $$r_{u,i} \\approx {\\bf f}_u^T\\cdot{\\bf f}_i$$\n",
    "\n",
    "The underlying assumption is that both users and items *live* in the same latent space, and that we can unravel such space. \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Tunca_Dogan/publication/235913413/figure/fig3/AS:299678856957952@1448460415040/Figure-3-The-distribution-of-the-points-in-the-Swiss-roll-dataset-in-3-D-space.png\n",
    "\" alt=\"swiss roll\" style=\"float: center; width: 300px\"/>\n",
    "\n",
    "\n",
    "Here we will use a couple of linear Matrix Factorization (MF) models:\n",
    "\n",
    "* Singular Value decomposition (SVD)\n",
    "* Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Singular value decomposition\n",
    "\n",
    "The main idea is to reduce the dimensionality of the input space. This is pretty much the same as Eigen-decomposition or Principal Component Analysis (PCA)-\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/220px-GaussianScatterPCA.svg.png\n",
    "\" alt=\"dimensionaly reducion\" style=\"float: center; width: 500px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the help!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get SVD components from train matrix. Choose k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# take a look at the different matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the recommendations\n",
    "\n",
    "We will reconstruct the utility matrix R (i.e., the recommendation matrix) as follows:\n",
    "\n",
    "### $$M\\approx U\\mathrm{diag}(s)V^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a diagonal matrix with the eigenvalues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions are correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "* RMSE\n",
    "* Recall@30\n",
    "* MAP@30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('SVD RMSE=%.3f' % #TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the zipped list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same with more dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit vs Explicit feedback\n",
    "\n",
    "In the above SVD matrix factorization we have tried to reconstructt the matrix of ratings. Howvever, it might be easier trying to model the matrix of preferences (i.e., wether the user likes or not a movie).\n",
    "\n",
    "For this, we will define a “selector” matrix $I$ for the training utility matrix $R$, which will contain 0 if the rating matrix has no rating entry, and 1 if the rating matrix contains an entry. And the smae for testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index matrix for training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index matrix for test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "As we introduce more dimensions in the model, we make it more prone to overfit. This can be observed in the decrease of error (RMSE) in train, while it increases in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(dims, error_train, '--*b', label=\"train\")\n",
    "plt.semilogx(dims, error_test, '--*g', label=\"test\")\n",
    "plt.xlabel(\"Numer of latent dimensions\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend(loc=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that after few dimensions (~10) the model starts to overfit (the error in test increases). Thus, we need to use other method to regularize the model (i.e., set restrictions/limitations to the model, so that it cannot overfit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Alternating Least Squares (ALS)\n",
    "\n",
    "SVD can be very slow and computationally expensive. Besides, when addressing only the relatively few known entries we are highly prone to overfitting.\n",
    "\n",
    "An scalable alternative to SVD is ALS, which can include regularization terms to prevent overfitting. We will rename our variable to make them more similar to the ALS notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS algorithm\n",
    "\n",
    "The ALS algorithm aims to estimate two unknown matrices which, when multiplied together, yield the rating matrix. The loss function you will use is the well-known sum of squared errors. The second term is for regularisation to prevent overfitting\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\underset{Q*&space;,&space;P*}{min}\\sum_{(u,i)\\epsilon&space;K&space;}(r_{ui}-P_u^TQ_i)^2&plus;\\lambda(\\left&space;\\|&space;Q_i&space;\\right&space;\\|^2&space;&plus;&space;\\left&space;\\|&space;P_u&space;\\right&space;\\|^2)$&space;&space;$\" title=\"\\underset{q* , p*}{min}\\sum_{(u,i)\\epsilon K }(r_{ui}-q_i^Tp_u)^2+\\lambda(\\left \\| q_i \\right \\|^2 + \\left \\| p_u \\right \\|^2)\" />\n",
    "\n",
    "The Alternating Least Squares algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name. \n",
    "\n",
    "<img alt=\"factorization\" src=\"http://spark-mooc.github.io/web-assets/images/matrix_factorization.png\" style=\"width: 885px\"/>\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "This optimization is what's being shown on the right in the image above.  Given a fixed set of user factors (i.e., values in the users matrix), we use the known ratings to find the best values for the movie factors using the optimization written at the bottom of the figure.  Then we \"alternate\" and pick the best user factors given fixed movie factors.\n",
    "\n",
    "It must be noticed that this is another way of reducing the dimensionality of the input matrix (like PCA, or more generally, SVD). This has important consequences:\n",
    "\n",
    "* ### Our decomposition is linear. We won't be able to catch non-linear relationships among users and items.\n",
    "* ### As in PCA or SVD, our features will correspond to directions of maximum variance in the data. Thus, the first feature will catch most of this variation, the second, a little bit more, and so on. It implies that the error in the reconstruction will not decrease dramatically when using more features!!! Keep this in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alsRmse(I,R,P,Q):\n",
    "    return np.sqrt(np.sum((I * (R - np.dot(P,Q.T)))**2)/len(R[R > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(user, fixed_vecs, counts, num_factors, reg_param, num_solve, verbose=False):\n",
    "    num_fixed = fixed_vecs.shape[0]\n",
    "    YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "    eye = np.eye(num_fixed)\n",
    "    lambda_eye = reg_param * np.eye(num_factors)\n",
    "    solve_vecs = np.zeros((num_solve, num_factors))\n",
    "\n",
    "    t = time.time()\n",
    "    for i in range(num_solve):\n",
    "        if user:\n",
    "            counts_i = counts[i]\n",
    "        else:\n",
    "            counts_i = counts[:, i].T\n",
    "        CuI = np.diag(counts_i)\n",
    "        pu = counts_i.copy()\n",
    "        pu[np.where(pu != 0)] = 1.0\n",
    "        YTCuIY = fixed_vecs.T.dot(CuI).dot(fixed_vecs)\n",
    "        YTCupu = fixed_vecs.T.dot(CuI + eye).dot(pu.T)\n",
    "        xu = np.linalg.solve(YTY + YTCuIY + lambda_eye, YTCupu)\n",
    "        solve_vecs[i] = xu\n",
    "        if verbose and i % 300 == 0:\n",
    "            print('Solved %i vecs in %d seconds' % (i, time.time() - t))\n",
    "            t = time.time()\n",
    "\n",
    "    return solve_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance by plotting train and test errors\n",
    "def check_als_performance(n_epochs, train_errors, test_errors):\n",
    "    plt.plot(range(n_epochs), train_errors, marker='o', label='Training Data');\n",
    "    plt.plot(range(n_epochs), test_errors, marker='v', label='Test Data');\n",
    "    plt.title('ALS-WR Learning Curve')\n",
    "    plt.xlabel('Number of Epochs');\n",
    "    plt.ylabel('RMSE');\n",
    "    plt.ylim(1, 5)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check als performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS evaluation\n",
    "* RMSE\n",
    "* recall@[5, 10, 20, 50, 100, 200, 500]\n",
    "* map@[5, 10, 20, 50, 100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('ALS CF RMSE: ' + str(rmse(alsPredictions, uMatrixTesting)))\n",
    "print('SVD CF RMSE: ' + str(rmse(svdPredictions, uMatrixTesting)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalutation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## 4. Exercises (advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**E1:** Implement centered cosine similarity metric in [CF with co-occurrence](#copurchase)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**E2:** Implement k-neighbors in [Memory-based CF](#memory-base)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**E3:** Implement ALS with user and items biases, i.e.\n",
    "\n",
    "$r_{ui}\\approx P_u^T*Q_i+b_u+b_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**E4:** Implement ALS in terms of implicit feedback (1s and 0s), instead of ratings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at http://infolab.stanford.edu/~ullman/mmds/ch9.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
